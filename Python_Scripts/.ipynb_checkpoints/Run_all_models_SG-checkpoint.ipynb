{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b91013cf-e407-46f0-9689-1ef31838cf92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\killi\\AppData\\Local\\Temp/ipykernel_3392/3397146392.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  mydata2['date'] = pd.to_datetime(mydata2['date'])\n",
      "C:\\Users\\killi\\AppData\\Local\\Temp/ipykernel_3392/3397146392.py:43: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  mydata2['date_round'] = mydata2['date'] + pd.to_timedelta(mydata2['Round'] - 4, unit='D')\n",
      "C:\\Users\\killi\\AppData\\Local\\Temp/ipykernel_3392/3397146392.py:49: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  mydata2['time'] = (mydata2['date_round'] - earliest_date).dt.days\n"
     ]
    }
   ],
   "source": [
    "# Section 1: Read in Packages\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "import pystan\n",
    "import arviz as az\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Section 2: Read in data - this step can stay the same\n",
    "\n",
    "# Read in model data - round level (37,676)\n",
    "model_data = pd.read_csv('C:\\\\KF_Repo\\\\PGA_Golf\\\\Tournament_level_model\\\\Data_manipulation\\\\model_data.csv')\n",
    "\n",
    "\n",
    "# Group by count using pandas groupby()\n",
    "grouped_data = model_data.groupby(['tournament id', 'Round'])['Round_total'].mean().reset_index()\n",
    "\n",
    "# Rename columns\n",
    "grouped_data = grouped_data.rename(columns={\"tournament id\": \"tournament id\", \n",
    "                                            \"Round\": \"Round\",\n",
    "                                            \"Round_total\": \"Avg_Score\"})\n",
    "\n",
    "# Round Avg Score to 2 decimal places (same as strokes gained)\n",
    "grouped_data['Avg_Score'] = grouped_data['Avg_Score'].round(2)\n",
    "\n",
    "# Merge dataframes by 'tournament.id' and 'Round'\n",
    "add_avg = pd.merge(model_data, grouped_data, on=['tournament id', 'Round'])\n",
    "\n",
    "\n",
    "# Add difference - put same format as strokes gained\n",
    "# Negative is bad, positive is good\n",
    "add_avg['Round_sg'] = add_avg['Avg_Score'] - add_avg['Round_total']\n",
    "\n",
    "# Filter data for players that you want to analyze\n",
    "my_players = ['Seamus Power', 'Tony Finau']\n",
    "mydata2 = add_avg[add_avg['player'].isin(my_players)]\n",
    "\n",
    "# Convert date to datetime format\n",
    "mydata2['date'] = pd.to_datetime(mydata2['date'])\n",
    "\n",
    "# Add in a column for date of round\n",
    "mydata2['date_round'] = mydata2['date'] + pd.to_timedelta(mydata2['Round'] - 4, unit='D')\n",
    "\n",
    "# Find the earliest date\n",
    "earliest_date = mydata2['date_round'].min()\n",
    "\n",
    "# Calculate the time column\n",
    "mydata2['time'] = (mydata2['date_round'] - earliest_date).dt.days\n",
    "\n",
    "# Create a sequence of unique dates and assign corresponding time values\n",
    "unique_dates = mydata2['date_round'].unique()\n",
    "date_to_time_mapping = pd.DataFrame({'date_round': unique_dates, 'time_2': np.arange(len(unique_dates))})\n",
    "\n",
    "# Merge the mapping with the original dataframe\n",
    "mydata2 = pd.merge(mydata2, date_to_time_mapping, on='date_round', how='left')\n",
    "\n",
    "# Concatenate columns with \"_\"\n",
    "unique_tr = mydata2[['tournament name','date_round', 'Round']].drop_duplicates()\n",
    "unique_tr['cr'] = unique_tr['tournament name'].astype(str) + \"_\"+ unique_tr['date_round'].astype(str) + \"_\" +\"R\"+ unique_tr['Round'].astype(str)\n",
    "\n",
    "# Concatenate columns with \"_\"\n",
    "unique_tourn = mydata2[['tournament name', 'date']].drop_duplicates()\n",
    "unique_tourn['tourn'] = unique_tourn['tournament name'].astype(str) + \"_\" + unique_tourn['date'].astype(str)\n",
    "\n",
    "# Create additional dataframe before filter\n",
    "mydata_all = pd.merge(mydata2, unique_tr, on=['tournament name','date_round', 'Round'], how='left')\n",
    "mydata_all = pd.merge(mydata_all, unique_tourn, on=['tournament name', 'date'], how='left')\n",
    "\n",
    "# Keep using mydata2\n",
    "mydata2 = pd.merge(mydata2, unique_tr, on=['tournament name','date_round', 'Round'], how='left')\n",
    "mydata2 = pd.merge(mydata2, unique_tourn, on=['tournament name', 'date'], how='left')\n",
    "\n",
    "\n",
    "# Filter train and test data\n",
    "train_data = mydata_all[mydata_all['date_round'] <= \"2020-08-30\"]\n",
    "test_data = mydata_all[mydata_all['date_round'] > \"2020-08-30\"]\n",
    "\n",
    "# Perform Regression on training data\n",
    "reg_data = train_data\n",
    "\n",
    "# Filter by player\n",
    "power = reg_data[reg_data['player'] == \"Seamus Power\"]\n",
    "\n",
    "# Order by date round\n",
    "power = power.sort_values(by='date_round')\n",
    "\n",
    "# Create a time series object\n",
    "ts_data = pd.Series(power['Round_sg'].values, index=power['date_round'])\n",
    "\n",
    "\n",
    "## partition into train and test\n",
    "train_series = ts_data[:80]\n",
    "test_series = ts_data[80:103]\n",
    "\n",
    "\n",
    "# Get observed scores to use for model\n",
    "observed_round_score = train_series.values\n",
    "\n",
    "model_data = {'N': len(observed_round_score),\n",
    "               'y': observed_round_score}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "da932bec-de26-4359-8b02-8d7037709588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 3: Create a dictionary with all of my model codes\n",
    "\n",
    "regression_model_code = \"\"\"\n",
    "data {\n",
    "  int<lower=0> N; //number of rows in training set\n",
    "  vector[N] y;\n",
    "} \n",
    "\n",
    "parameters {  \n",
    "  \n",
    "// golfer parameters\n",
    "  real golfer_estimate;\n",
    "\n",
    "// residual error in likelihood\n",
    "  real<lower=0.00001> sigma_y;\n",
    "} \n",
    "\n",
    "model {\n",
    "      // Set priors     \n",
    "      sigma_y ~ normal(0, 100); \n",
    "      \n",
    "    // Likelihood\n",
    "    y ~ normal(golfer_estimate, sigma_y);\n",
    "}\n",
    "\n",
    "\n",
    "generated quantities {\n",
    " // generate simulated values for y\n",
    "  vector[N] y_sim;\n",
    "  for (i in 1:N)\n",
    "  y_sim[i] = normal_rng(golfer_estimate, sigma_y);\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "skew_normal_model_code = \"\"\"\n",
    "data {\n",
    "  int<lower=0> N; //number of rows in training set\n",
    "  vector[N] y;\n",
    "} \n",
    "\n",
    "parameters {  \n",
    "  \n",
    "// parameters - shape is skew - scale has to be positive\n",
    "  real location;\n",
    "  real <lower=0.00001> scale;\n",
    "  real shape;\n",
    "\n",
    "} \n",
    "\n",
    "model {\n",
    "  \n",
    "    // Likelihood\n",
    "    y ~ skew_normal(location, scale, shape);\n",
    "}\n",
    "\n",
    "generated quantities {\n",
    " // generate simulated values for y\n",
    "  vector[N] y_sim;\n",
    "  for (i in 1:N)\n",
    "  y_sim[i] = skew_normal_rng(location, scale, shape);\n",
    "}\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "cauchy_model_code = \"\"\"\n",
    "data {\n",
    "  int<lower=0> N; //number of rows in training set\n",
    "  vector[N] y;\n",
    "} \n",
    "\n",
    "parameters {  \n",
    "  \n",
    "// parameters - shape is skew - scale has to be positive\n",
    "  real mu;\n",
    "  real <lower=0.00001> sigma;\n",
    "} \n",
    "\n",
    "model {\n",
    "  \n",
    "    // Likelihood\n",
    "    y ~ cauchy(mu, sigma);\n",
    "}\n",
    "\n",
    "generated quantities {\n",
    " // generate simulated values for y\n",
    "  vector[N] y_sim;\n",
    "  for (i in 1:N)\n",
    "  y_sim[i] = cauchy_rng(mu, sigma);\n",
    "}\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "double_exp_code = \"\"\"\n",
    "data {\n",
    "  int<lower=0> N; //number of rows in training set\n",
    "  vector[N] y;\n",
    "} \n",
    "\n",
    "parameters {  \n",
    "  \n",
    "// parameters - shape is skew - scale has to be positive\n",
    "  real mu;\n",
    "  real <lower=0.00001> sigma;\n",
    "} \n",
    "\n",
    "model {\n",
    "  \n",
    "    // Likelihood\n",
    "    y ~ double_exponential(mu, sigma);\n",
    "}\n",
    "\n",
    "generated quantities {\n",
    " // generate simulated values for y\n",
    "  vector[N] y_sim;\n",
    "  for (i in 1:N)\n",
    "  y_sim[i] = double_exponential_rng(mu, sigma);\n",
    "}\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "logistic_model_code = \"\"\"\n",
    "data {\n",
    "  int<lower=0> N; //number of rows in training set\n",
    "  vector[N] y;\n",
    "} \n",
    "\n",
    "parameters {  \n",
    "  \n",
    "// parameters - shape is skew - scale has to be positive\n",
    "  real mu;\n",
    "  real <lower=0.00001> sigma;\n",
    "} \n",
    "\n",
    "model {\n",
    "  \n",
    "    // Likelihood\n",
    "    y ~ logistic(mu, sigma);\n",
    "}\n",
    "\n",
    "generated quantities {\n",
    " // generate simulated values for y\n",
    "  vector[N] y_sim;\n",
    "  for (i in 1:N)\n",
    "  y_sim[i] = logistic_rng(mu, sigma);\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "gumbel_model_code = \"\"\"\n",
    "data {\n",
    "  int<lower=0> N; //number of rows in training set\n",
    "  vector[N] y;\n",
    "} \n",
    "\n",
    "parameters {  \n",
    "  \n",
    "// parameters - shape is skew - scale has to be positive\n",
    "  real mu;\n",
    "  real <lower=0.00001> sigma;\n",
    "} \n",
    "\n",
    "model {\n",
    "  \n",
    "    // Likelihood\n",
    "    y ~ gumbel(mu, sigma);\n",
    "}\n",
    "\n",
    "generated quantities {\n",
    " // generate simulated values for y\n",
    "  vector[N] y_sim;\n",
    "  for (i in 1:N)\n",
    "  y_sim[i] = gumbel_rng(mu, sigma);\n",
    "}\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "gen_normal_model_code = \"\"\"\n",
    "\n",
    "functions {\n",
    "    real generalized_normal_log(vector x, real mu, real alpha, real beta) {\n",
    "        vector[num_elements(x)] log_prob;\n",
    "        real lprob;\n",
    "        for (i in 1:num_elements(x)) {\n",
    "            log_prob[i] = log(beta/(2 * alpha * tgamma(1 / beta))) -abs((x[i] - mu) / alpha)^beta;\n",
    "        }\n",
    "        lprob = sum(log_prob);\n",
    "        return lprob;\n",
    "    }\n",
    "    \n",
    "     real sign(real x) {\n",
    "        if (x < 0) {\n",
    "            return -1.0;\n",
    "        } else if (x > 0) {\n",
    "            return 1.0;\n",
    "        } else {\n",
    "            return 0.0;\n",
    "        }\n",
    "    }\n",
    "  \n",
    "      // this formula seems to give the prediction with correct std dev\n",
    "      real generalized_normal_inv_cdf(real p, real mu, real alpha, real beta) {\n",
    "        real quantile;\n",
    "   \n",
    "        // Compute the regularized lower incomplete gamma function\n",
    "        real P = gamma_p(1.0 / beta, pow(-log(1.0 - fabs(p - 0.5)), 1.0 / beta));\n",
    "\n",
    "        // Compute the quantile\n",
    "        quantile = mu + alpha * sign(p - 0.5) * P;\n",
    "\n",
    "        return quantile;\n",
    "    }\n",
    "  \n",
    "  \n",
    "  // Create function to generate random numbers from the generalised normal\n",
    "    real generalized_normal_rng(real mu, real alpha, real beta){\n",
    "    real u = uniform_rng(0, 1);\n",
    "    return generalized_normal_inv_cdf(u, mu,alpha, beta );\n",
    "  }\n",
    "\n",
    "}\n",
    "\n",
    "data {\n",
    "    int<lower=0> N;            // Number of data points\n",
    "    vector[N] y;               // Data vector\n",
    "}\n",
    "\n",
    "parameters {\n",
    "    real mu;                   // Mean parameter\n",
    "    real<lower=0> alpha;       // Scale parameter (standard deviation)\n",
    "    real<lower=0> beta;        // Shape parameter\n",
    "}\n",
    "\n",
    "model {\n",
    "    // Prior distributions for parameters\n",
    "    mu ~ normal(0, 10);        // Weakly informative prior for mean\n",
    "    alpha ~ cauchy(0, 5);      // Weakly informative prior for scale\n",
    "    beta ~ gamma(1, 1);        // Weakly informative prior for shape\n",
    "    \n",
    "    // Likelihood function - used to get estimates of parameters\n",
    "    y ~  generalized_normal(mu,alpha, beta);\n",
    "}\n",
    "\n",
    "\n",
    "generated quantities {\n",
    " // generate simulated values for y\n",
    "  vector[N] y_sim;\n",
    "  for (i in 1:N)\n",
    "  y_sim[i] = generalized_normal_rng(mu, alpha, beta); ;\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# Store the model code in a dictionary\n",
    "model_dictionary = {\n",
    "    \"regression_model\": regression_model_code,\n",
    "    \"skew_normal_model\": skew_normal_model_code,\n",
    "    \"cauchy_model\": cauchy_model_code,\n",
    "     \"double_exponential_model\": double_exp_code,\n",
    "    \"logistic_model\": logistic_model_code,\n",
    "    \"gumbel_model\": gumbel_model_code,\n",
    "    \"generalised_normal_model\": gen_normal_model_code\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9abcd2ca-8ac2-40d7-8272-4f50d39ea33c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pystan:COMPILING THE C++ CODE FOR MODEL anon_model_00703864a8c8aa169afb232794c36fb9 NOW.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running regression_model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pystan:COMPILING THE C++ CODE FOR MODEL anon_model_2dea693dfa3f9d28d75b43d3912317ef NOW.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running skew_normal_model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pystan:COMPILING THE C++ CODE FOR MODEL anon_model_8d0f45ebb3d630f94bb2c0f6a41bdfd0 NOW.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running cauchy_model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pystan:COMPILING THE C++ CODE FOR MODEL anon_model_c38954fb3283bb7ed2ef11163fd84af6 NOW.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running double_exponential_model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pystan:COMPILING THE C++ CODE FOR MODEL anon_model_8053d3a139a784423734ab75dfa79bb4 NOW.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running logistic_model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pystan:COMPILING THE C++ CODE FOR MODEL anon_model_c1dcdbb23ed2e0fb27770e039649d1cb NOW.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running gumbel_model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pystan:COMPILING THE C++ CODE FOR MODEL anon_model_3e9b27239d90925777f517a60b2d5bf6 NOW.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running uniform_model...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Initialization failed.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"C:\\Users\\killi\\anaconda3\\envs\\stan_2\\lib\\multiprocessing\\pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n  File \"C:\\Users\\killi\\anaconda3\\envs\\stan_2\\lib\\multiprocessing\\pool.py\", line 48, in mapstar\n    return list(map(*args))\n  File \"stanfit4anon_model_3e9b27239d90925777f517a60b2d5bf6_4285933495470809889.pyx\", line 373, in stanfit4anon_model_3e9b27239d90925777f517a60b2d5bf6_4285933495470809889._call_sampler_star\n  File \"stanfit4anon_model_3e9b27239d90925777f517a60b2d5bf6_4285933495470809889.pyx\", line 406, in stanfit4anon_model_3e9b27239d90925777f517a60b2d5bf6_4285933495470809889._call_sampler\nRuntimeError: Initialization failed.\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3392/3007210895.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[1;31m# Call sampling function with data as argument\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[1;31m# Use 2 chains with 1k iterations each - 500 burn in - this will give 1k\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m     \u001b[0mfit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstan_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msampling\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchains\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[1;31m# Put Posterior draws into a dictionary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\stan_2\\lib\\site-packages\\pystan\\model.py\u001b[0m in \u001b[0;36msampling\u001b[1;34m(self, data, pars, chains, iter, warmup, thin, seed, init, sample_file, diagnostic_file, verbose, algorithm, control, n_jobs, **kwargs)\u001b[0m\n\u001b[0;32m    811\u001b[0m         \u001b[0mcall_sampler_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mizip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitertools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitertools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    812\u001b[0m         \u001b[0mcall_sampler_star\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_sampler_star\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 813\u001b[1;33m         \u001b[0mret_and_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_map_parallel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_sampler_star\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcall_sampler_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    814\u001b[0m         \u001b[0msamples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0msmpl\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msmpl\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mret_and_samples\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    815\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\stan_2\\lib\\site-packages\\pystan\\model.py\u001b[0m in \u001b[0;36m_map_parallel\u001b[1;34m(function, args, n_jobs)\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m             \u001b[0mpool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmultiprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocesses\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m             \u001b[0mmap_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m             \u001b[0mpool\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\stan_2\\lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36mmap\u001b[1;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[0;32m    362\u001b[0m         \u001b[1;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mthat\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mreturned\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    363\u001b[0m         '''\n\u001b[1;32m--> 364\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_map_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmapstar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    365\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    366\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstarmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\stan_2\\lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    769\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    770\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 771\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    772\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    773\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_set\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Initialization failed."
     ]
    }
   ],
   "source": [
    "# Section 4\n",
    "# Build a Stan Model using a simple regression\n",
    "\n",
    "# Function to count how many times values in statistics[key][1] are greater than in statistics[key][0]\n",
    "def count_greater_values(stats):\n",
    "    counts = {}\n",
    "    for key, (val0, val1) in stats.items():\n",
    "        count = np.sum(val1 > val0)\n",
    "        percentage = round(((count / len(val1)) * 100),2)\n",
    "        ppp_value = count / len(val1)  # Posterior predictive p-value\n",
    "        # Determine the alert level based on the PPP-value\n",
    "        if ppp_value < 0.05 or ppp_value > 0.95:\n",
    "            alert = \"Red\"\n",
    "        elif 0.05 <= ppp_value < 0.1 or 0.9 < ppp_value <= 0.95:\n",
    "            alert = \"Amber\"\n",
    "        elif 0.45 <= ppp_value <= 0.55:\n",
    "            alert = \"Ideal\"\n",
    "        else:\n",
    "            alert = \"Acceptable\"\n",
    "        counts[key] = (count, percentage, alert)\n",
    "    return counts\n",
    "\n",
    "# Function to apply color coding\n",
    "def color_code_alert(val):\n",
    "    color = ''\n",
    "    if val == \"Ideal\":\n",
    "        color = 'lightgreen'\n",
    "    elif val == \"Acceptable\":\n",
    "        color = 'cyan'\n",
    "    elif val == \"Amber\":\n",
    "        color = 'orange'\n",
    "    elif val == \"Red\":\n",
    "        color = 'red'\n",
    "    return f'background-color: {color}'\n",
    "\n",
    "\n",
    "\n",
    "# Empty dictionary to store results\n",
    "results = {}\n",
    "\n",
    "# Loop through each model in model_dictionary\n",
    "for model_name, model_code in model_dictionary.items():\n",
    "    print(f\"Running {model_name}...\")\n",
    "\n",
    "\n",
    "    # Create Model - this will help with recompilation issues\n",
    "    stan_model = pystan.StanModel(model_code=model_code)\n",
    "\n",
    "    # Call sampling function with data as argument\n",
    "    # Use 2 chains with 1k iterations each - 500 burn in - this will give 1k\n",
    "    fit = stan_model.sampling(data=model_data, iter=10, chains=2, seed=1)\n",
    "\n",
    "    # Put Posterior draws into a dictionary\n",
    "    trace = fit.extract()\n",
    "\n",
    "    # Put simulations into an array\n",
    "    # Need this for plotting graphs\n",
    "    y_sim = trace['y_sim']\n",
    "\n",
    "    # Create summary dictionary\n",
    "    summary_dict = fit.summary()\n",
    "\n",
    "    # get trace summary - need this for checkin rhat close to 1\n",
    "    trace_summary = pd.DataFrame(summary_dict['summary'], \n",
    "                  columns=summary_dict['summary_colnames'], \n",
    "                  index=summary_dict['summary_rownames'])\n",
    "\n",
    "\n",
    "    # count how many values from the trace summary are greater than 1.1\n",
    "    count_rhat_greater_1_1 = np.sum(trace_summary['Rhat'] > 1.1)\n",
    "\n",
    "    # Section 4: Posterior Predictive P values\n",
    "    # Probability that the replicated data is more extreme than the actual data\n",
    "\n",
    "    # Define statistics to loop through\n",
    "    # Will have 1k values for each value \n",
    "    # E.g. Taking the min of the 80 observations 1k times\n",
    "    statistics = {\n",
    "        'min': (np.min(observed_round_score), np.min(y_sim, axis=1)),\n",
    "        '2.5%': (np.percentile(observed_round_score,2.5),np.percentile(y_sim,2.5,axis=1)),\n",
    "        '25%': (np.percentile(observed_round_score,25),np.percentile(y_sim,25,axis=1)),\n",
    "        '50%': (np.percentile(observed_round_score,50),np.percentile(y_sim,50,axis=1)),\n",
    "        'mean': (np.mean(observed_round_score), np.mean(y_sim, axis=1)),\n",
    "        '75%': (np.percentile(observed_round_score,75),np.percentile(y_sim,75,axis=1)),\n",
    "        '97.5%': (np.percentile(observed_round_score,97.5),np.percentile(y_sim,97.5,axis=1)),\n",
    "        'max': (np.max(observed_round_score), np.max(y_sim, axis=1)),\n",
    "        'std': (np.std(observed_round_score), np.std(y_sim, axis=1))\n",
    "        }\n",
    "\n",
    "    # Find how many times the test statistic in simulated dataset exceeds actual data\n",
    "\n",
    "    # Get the counts\n",
    "    counts = count_greater_values(statistics)\n",
    "\n",
    "    # Convert the counts dictionary to a DataFrame\n",
    "    df_counts = pd.DataFrame(counts).T.reset_index()\n",
    "    df_counts.columns = ['Statistic', 'Count', 'Percentage', 'Alert']\n",
    "    \n",
    "    # Add Model column to df_counts\n",
    "    df_counts['Model'] = model_name\n",
    "    \n",
    "    # Store results in dictionary\n",
    "    results[model_name] = {\n",
    "        'count_rhat_greater_1_1': count_rhat_greater_1_1,\n",
    "        'df_counts': df_counts\n",
    "    }\n",
    "\n",
    "\n",
    "# Example: Print the summary statistics for each model\n",
    "for model_name, result in results.items():\n",
    "    print(f\"\\nModel: {model_name}\")\n",
    "    print(f\"Count of Rhat > 1.1: {result['count_rhat_greater_1_1']}\")\n",
    "\n",
    "    # Display styled dataframe\n",
    "    df_counts_styled = result['df_counts'].style.applymap(color_code_alert, subset=['Alert'])\n",
    "    df_counts_styled = df_counts_styled.format({'Percentage': '{:.2f}'})\n",
    "    display(df_counts_styled)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269dc9be-d303-4a1d-bf10-529333f2c738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 5: Plot Posterior Predictive Checks\n",
    "\n",
    "# Plot data\n",
    "for stat_name, (data_value, sim_values) in statistics.items():\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.axvline(x=data_value, color='r', linestyle='--', label=f'{stat_name}_data Line')\n",
    "    plt.hist(sim_values, bins=20, alpha=0.7, label=f'{stat_name}_sim Histogram')\n",
    "    plt.xlabel('Values')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title(f'Comparison of {stat_name.capitalize()} Data and Simulations')\n",
    "    plt.legend()\n",
    "    plt.grid()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88031f5-8da2-4be3-939f-13d23bb3bcba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
